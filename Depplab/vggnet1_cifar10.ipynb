{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pand\n",
    "import tensorflow as tf\n",
    "import os, random\n",
    "import pickle\n",
    "from glob import glob\n",
    "from time import localtime, strftime\n",
    "from collections import OrderedDict\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data_folder='data/cifar10/'\n",
    "train_file=['data_batch_1','data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5']\n",
    "test_file=['data/cifar10/test_batch']\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK READY\n"
     ]
    }
   ],
   "source": [
    "# NETWORK TOPOLOGIES\n",
    "list_class=['airplane', 'automobile', 'birds', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "n_input    = 32\n",
    "n_TotalInput=n_input*n_input\n",
    "n_channel=3\n",
    "\n",
    "n_NumDense=n_TotalInput//32 #3 pooling layer\n",
    "n_classes  = len(list_class)  \n",
    "n_layer=3\n",
    "\n",
    "# INPUTS AND OUTPUTS\n",
    "x = tf.placeholder(\"float\", [100, n_TotalInput*n_channel])\n",
    "y = tf.placeholder(\"float\", [100, n_classes])\n",
    "    \n",
    "# NETWORK PARAMETERS\n",
    "stddev = 0.1\n",
    "weights = {\n",
    "    'c1': tf.Variable(tf.random_normal([3, 3, n_channel, 32], stddev=stddev)),\n",
    "    'c2': tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=stddev)),\n",
    "    'c3': tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=stddev)),\n",
    "    'd1': tf.Variable(tf.random_normal([16*128, n_classes], stddev=stddev))\n",
    "}\n",
    "biases = {\n",
    "    'c1': tf.Variable(tf.random_normal([32], stddev=stddev)),\n",
    "    'c2': tf.Variable(tf.random_normal([64], stddev=stddev)),\n",
    "    'c3': tf.Variable(tf.random_normal([128], stddev=stddev)),\n",
    "    'd1': tf.Variable(tf.random_normal([n_classes], stddev=stddev))\n",
    "}\n",
    "print (\"NETWORK READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "def VGGNet(img_width, img_height, img_channel, _x, _w, _b, scope='Simple_VGG'):\n",
    "    \n",
    "    _names = OrderedDict()\n",
    "    \n",
    "    \n",
    "    # RESHAPE\n",
    "    _x_r = tf.reshape(_x, shape=[-1,img_width,img_height, img_channel])\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        # CONVOLUTION\n",
    "        \n",
    "        _conv1 = tf.nn.conv2d(_x_r, _w['c1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        # ADD BIAS\n",
    "        _conv1 = tf.nn.bias_add(_conv1, _b['c1'])\n",
    "        # RELU\n",
    "        _conv1 = tf.nn.relu(_conv1)\n",
    "        _names['Conv3x3_1'] = _conv1\n",
    "        # MAX-POOL\n",
    "        _pool  = tf.nn.max_pool(_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        _names['pool1'] = _pool\n",
    "        \n",
    "        conv2 = tf.nn.conv2d(_pool, _w['c2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv2 = tf.nn.bias_add(conv2, _b['c2'])\n",
    "        conv2 = tf.nn.relu(conv2)\n",
    "        _names['Conv3x3_2'] = conv2\n",
    "        pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        _names['pool2'] = pool2\n",
    "        \n",
    "        \n",
    "        conv3 = tf.nn.conv2d(pool2, _w['c3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv3 = tf.nn.bias_add(conv3, _b['c3'])\n",
    "        conv3 = tf.nn.relu(conv3)\n",
    "        _names['Conv3x3_3'] = conv3\n",
    "        pool3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        _names['pool3'] = pool3\n",
    "        \n",
    "        \n",
    "        # VECTORIZE\n",
    "        _dense = tf.reshape(pool3, [-1, _w['d1'].get_shape().as_list()[0]])\n",
    "        _names['dense'] = _dense\n",
    "        # DENSE\n",
    "        _logit = tf.add(tf.matmul(_dense, _w['d1']), _b['d1'])\n",
    "        \n",
    "        _names['logit'] = _logit\n",
    "            \n",
    "    return _names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Conv3x3_1', <tf.Tensor 'Simple_VGG/Relu:0' shape=(100, 32, 32, 32) dtype=float32>), ('pool1', <tf.Tensor 'Simple_VGG/MaxPool:0' shape=(100, 16, 16, 32) dtype=float32>), ('Conv3x3_2', <tf.Tensor 'Simple_VGG/Relu_1:0' shape=(100, 16, 16, 64) dtype=float32>), ('pool2', <tf.Tensor 'Simple_VGG/MaxPool_1:0' shape=(100, 8, 8, 64) dtype=float32>), ('Conv3x3_3', <tf.Tensor 'Simple_VGG/Relu_2:0' shape=(100, 8, 8, 128) dtype=float32>), ('pool3', <tf.Tensor 'Simple_VGG/MaxPool_2:0' shape=(100, 4, 4, 128) dtype=float32>), ('dense', <tf.Tensor 'Simple_VGG/Reshape:0' shape=(100, 2048) dtype=float32>), ('logit', <tf.Tensor 'Simple_VGG/Add:0' shape=(100, 10) dtype=float32>)])\n",
      "FUNCTIONS READY\n"
     ]
    }
   ],
   "source": [
    "# PREDICTION\n",
    "cnnout = VGGNet(n_input, n_input, n_channel, x, weights, biases,'Simple_VGG')\n",
    "print(cnnout)\n",
    "# LOSS AND OPTIMIZER\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=cnnout['logit']))\n",
    "optm = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost) \n",
    "corr = tf.equal(tf.argmax(cnnout['logit'], 1), tf.argmax(y, 1))    \n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "\n",
    "# INITIALIZER\n",
    "init = tf.global_variables_initializer()\n",
    "print (\"FUNCTIONS READY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", shape=(100, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Simple_VGG/Reshape:0\", shape=(100, 2048), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(cnnout['dense'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVER READY\n"
     ]
    }
   ],
   "source": [
    "savedir = \"nets/cnn_cifar10_vgg/\"\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "save_step = 4\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "print (\"SAVER READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Conv3x3_1', <tf.Tensor 'Simple_VGG/Relu:0' shape=(100, 32, 32, 32) dtype=float32>), ('pool1', <tf.Tensor 'Simple_VGG/MaxPool:0' shape=(100, 16, 16, 32) dtype=float32>), ('Conv3x3_2', <tf.Tensor 'Simple_VGG/Relu_1:0' shape=(100, 16, 16, 64) dtype=float32>), ('pool2', <tf.Tensor 'Simple_VGG/MaxPool_1:0' shape=(100, 8, 8, 64) dtype=float32>), ('Conv3x3_3', <tf.Tensor 'Simple_VGG/Relu_2:0' shape=(100, 8, 8, 128) dtype=float32>), ('pool3', <tf.Tensor 'Simple_VGG/MaxPool_2:0' shape=(100, 4, 4, 128) dtype=float32>), ('dense', <tf.Tensor 'Simple_VGG/Reshape:0' shape=(100, 2048) dtype=float32>), ('logit', <tf.Tensor 'Simple_VGG/Add:0' shape=(100, 10) dtype=float32>)])\n"
     ]
    }
   ],
   "source": [
    "print(cnnout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_npz(file):\n",
    "  \n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Make batch_iterator for npz data\n",
    "def make_batch_index(_data, batch_size=128, allow_small_batch=True):\n",
    "    num_images = len(_data[b'data'])\n",
    "    start_idx = list(range(0, num_images, batch_size))\n",
    "    end_idx = list(range(batch_size, num_images + 1, batch_size))\n",
    "    if allow_small_batch and end_idx[-1] < num_images :\n",
    "        start_idx.append(end_idx[-1])\n",
    "        end_idx.append(num_images)\n",
    "    return zip(start_idx, end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(x, numclass):\n",
    "    return np.eye(numclass)[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 1000\n",
    "batch_size      = 100\n",
    "disp_each       = 10\n",
    "disp_batch      =100\n",
    "# LAUNCH THE GRAPH\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/cifar10/test_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fc03d4c789aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mload_testdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_npz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-45dd08475c1d>\u001b[0m in \u001b[0;36mload_npz\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_npz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bytes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/cifar10/test_batch'"
     ]
    }
   ],
   "source": [
    "load_testdata=load_npz(test_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    sample_list_new = random.sample(train_file, len(train_file))\n",
    "    nNumBatch=0;\n",
    "    AvgBatchCost=0\n",
    "    \n",
    "    for index, file_name in enumerate(sample_list_new):\n",
    "        file_name=data_folder+file_name\n",
    "        load_data=load_npz(file_name)\n",
    "        batch_index=make_batch_index(load_data,batch_size, True)\n",
    "        \n",
    "        \n",
    "        for start, end in batch_index:\n",
    "            \n",
    "            batch_xs = load_data[b'data'][start:end]\n",
    "            batch_ys = one_hot_encode(load_data[b'labels'][start:end],n_classes)\n",
    "            #print(batch_ys)\n",
    "            #print(batch_xs.shape)\n",
    "            feeds = {x: batch_xs, y: batch_ys}\n",
    "            sess.run(optm, feed_dict=feeds)\n",
    "            nNumBatch+=1\n",
    "            tmp_cost, _ = sess.run([cost, optm], feed_dict=feeds)\n",
    "            AvgBatchCost += tmp_cost\n",
    "            \n",
    "        if nNumBatch%disp_batch==0:\n",
    "                train_acc = sess.run(accr, feed_dict=feeds)\n",
    "                print('\\t[%d nNumBatch] train_cost = %g, acc=%g'%(nNumBatch, AvgBatchCost/nNumBatch, train_acc))\n",
    "           \n",
    "    if epoch%disp_each==0:\n",
    "        num_batch=0\n",
    "        test_acc=0\n",
    "        avg_acc=0\n",
    "        load_testdata=load_npz(test_file[0])\n",
    "        testdatalabel=one_hot_encode(load_testdata[b'labels'],n_classes)\n",
    "        test_batch_index=make_batch_index(load_data,batch_size, True)\n",
    "        for start, end in test_batch_index:\n",
    "            num_batch+=1;\n",
    "            batch_xs = load_testdata[b'data'][start:end]\n",
    "            batch_ys = one_hot_encode(load_testdata[b'labels'][start:end],n_classes)\n",
    "            test_acc = sess.run(accr, feed_dict={x:batch_xs,y:batch_ys})\n",
    "            avg_acc+=test_acc\n",
    "        print('[%d epoch]test acc=%g'%(epoch, avg_acc/num_batch))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for start, end in test_batch_index:\n",
    "    print(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
